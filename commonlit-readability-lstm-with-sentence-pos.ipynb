{"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install syllapy\n!pip install ety","metadata":{},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: syllapy in c:\\users\\focus\\develop\\cu\\dtsa-5511\\final\\.venv\\lib\\site-packages (0.7.2)\n\nRequirement already satisfied: ety in c:\\users\\focus\\develop\\cu\\dtsa-5511\\final\\.venv\\lib\\site-packages (1.4.0)\n\nRequirement already satisfied: treelib in c:\\users\\focus\\develop\\cu\\dtsa-5511\\final\\.venv\\lib\\site-packages (from ety) (1.7.0)\n\nRequirement already satisfied: colorful in c:\\users\\focus\\develop\\cu\\dtsa-5511\\final\\.venv\\lib\\site-packages (from ety) (0.5.5)\n\nRequirement already satisfied: six in c:\\users\\focus\\develop\\cu\\dtsa-5511\\final\\.venv\\lib\\site-packages (from ety) (1.16.0)\n\nRequirement already satisfied: colorama in c:\\users\\focus\\develop\\cu\\dtsa-5511\\final\\.venv\\lib\\site-packages (from colorful->ety) (0.4.6)\n"}]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport nltk\nfrom collections import Counter\nimport syllapy\nfrom nltk.corpus import wordnet\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport ety\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nfrom transformers import (\n    BertTokenizer,\n    TFBertModel,\n    XLMRobertaTokenizer,\n    TFXLMRobertaModel,\n)\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import (\n    Embedding,\n    Bidirectional,\n    LSTM,\n    Dense,\n    Dropout,\n    Input,\n    Flatten,\n    concatenate,\n    BatchNormalization,\n)\nfrom keras import regularizers\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model\n\ntqdm.pandas()\nnltk.download(\"wordnet\")\nnltk.download(\"punkt\")\nnltk.download(\"averaged_perceptron_tagger\")","metadata":{},"execution_count":2,"outputs":[{"name":"stderr","output_type":"stream","text":"c:\\Users\\focus\\Develop\\CU\\DTSA-5511\\final\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n\n  from .autonotebook import tqdm as notebook_tqdm\n\n[nltk_data] Downloading package wordnet to\n\n[nltk_data]     C:\\Users\\focus\\AppData\\Roaming\\nltk_data...\n\n[nltk_data]   Package wordnet is already up-to-date!\n\n[nltk_data] Downloading package punkt to\n\n[nltk_data]     C:\\Users\\focus\\AppData\\Roaming\\nltk_data...\n\n[nltk_data]   Package punkt is already up-to-date!\n\n[nltk_data] Downloading package averaged_perceptron_tagger to\n\n[nltk_data]     C:\\Users\\focus\\AppData\\Roaming\\nltk_data...\n\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n\n[nltk_data]       date!\n"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{}}]},{"cell_type":"markdown","source":"## Read and Process Data","metadata":{}},{"cell_type":"markdown","source":"Below, I pre-process the text data along with performing feature extraction.  The following code takes text input, performs various operations on it, and returns a set of linguistic statistics and processed data.\n\nThe following code:\n\n- Converts the input text to lowercase\n- Tokenizes the text into sentences using the Natural Language Toolkit (`nltk`), and each sentence is further tokenized into words. POS tags are assigned to each word.\n- For each word in the text, the code does the following:\n  - Determines the word's origin language (etymology).\n  - Retrieves the full name of the POS tag from the `pos_mapping`.\n  - Counts the number of syllables in the word.\n  - Calculates the length of the word.\n- Calculates various linguistic statistics, such as the mean syllable count, the number of sentences, the mean sentence length, the mean word length, and the total number of words.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\n    \"/kaggle/input/clear-corpus-6-01-clear-corpus-6-01/CLEAR Corpus 6.01 - CLEAR Corpus 6.01.csv\"\n)","metadata":{},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"pos_mapping = {\n    \"CC\": \"Coordinating Conjunction\",\n    \"CD\": \"Cardinal Digit\",\n    \"DT\": \"Determiner\",\n    \"EX\": \"Existential There\",\n    \"FW\": \"Foreign Word\",\n    \"IN\": \"Preposition or Subordinating Conjunction\",\n    \"JJ\": \"Adjective\",\n    \"JJR\": \"Adjective, Comparative\",\n    \"JJS\": \"Adjective, Superlative\",\n    \"LS\": \"List Item Marker\",\n    \"MD\": \"Modal\",\n    \"NN\": \"Noun, Singular or Mass\",\n    \"NNS\": \"Noun, Plural\",\n    \"NNP\": \"Proper Noun, Singular\",\n    \"NNPS\": \"Proper Noun, Plural\",\n    \"PDT\": \"Predeterminer\",\n    \"POS\": \"Possessive Ending\",\n    \"PRP\": \"Personal Pronoun\",\n    \"PRP$\": \"Possessive Pronoun\",\n    \"RB\": \"Adverb\",\n    \"RBR\": \"Adverb, Comparative\",\n    \"RBS\": \"Adverb, Superlative\",\n    \"RP\": \"Particle\",\n    \"TO\": \"to\",\n    \"UH\": \"Interjection\",\n    \"VB\": \"Verb, Base Form\",\n    \"VBD\": \"Verb, Past Tense\",\n    \"VBG\": \"Verb, Gerund or Present Participle\",\n    \"VBN\": \"Verb, Past Participle\",\n    \"VBP\": \"Verb, Non-3rd Person Singular Present\",\n    \"VBZ\": \"Verb, 3rd Person Singular Present\",\n    \"WDT\": \"Wh-determiner\",\n    \"WP\": \"Wh-pronoun\",\n    \"WP$\": \"Possessive Wh-pronoun\",\n    \"WRB\": \"Wh-adverb\",\n}\n\n\ndef process_text(text):\n    text = text.lower()\n\n    word_origins = []\n    word_pos = []\n    syllable_counts = []\n    sentence_lengths = []\n    word_lengths = []\n\n    sentences = sent_tokenize(text)\n\n    for sentence in sentences:\n        tokens = nltk.word_tokenize(sentence)\n        pos_tags = nltk.pos_tag(tokens)\n        sentence_lengths.append(len(pos_tags))\n        for token, pos in pos_tags:\n            origin = ety.origins(token)\n            if origin:\n                origin = origin[0].language.name\n            else:\n                origin = \"unknown\"\n            word_origins.append(origin)\n            full_pos_name = pos_mapping.get(pos, pos)\n            word_pos.append(full_pos_name)\n            syllables = syllapy.count(token)\n            syllable_counts.append(syllables)\n            word_lengths.append(len(token))\n\n    processed_excerpt = text\n    origin_counts = Counter(word_origins)\n    pos_counts = Counter(word_pos)\n    mean_syllable_count = np.mean(syllable_counts)\n    num_sentences = len(sentences)\n    mean_sentence_length = np.mean(sentence_lengths)\n    num_words = np.sum(sentence_lengths)\n    mean_word_length = np.mean(word_lengths)\n\n    return (\n        word_origins,\n        origin_counts,\n        word_pos,\n        pos_counts,\n        syllable_counts,\n        mean_syllable_count,\n        num_sentences,\n        mean_sentence_length,\n        mean_word_length,\n        num_words,\n        processed_excerpt,\n    )","metadata":{},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df[\n    [\n        \"word_origins\",\n        \"word_origin_counts\",\n        \"pos\",\n        \"pos_counts\",\n        \"syllable_counts\",\n        \"mean_syllable_count\",\n        \"num_sentences\",\n        \"mean_sentence_length\",\n        \"mean_word_length\",\n        \"num_words\",\n        \"processed_excerpt\",\n    ]\n] = df[\"Excerpt\"].progress_apply(lambda x: pd.Series(process_text(x)))","metadata":{},"execution_count":5,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 4724/4724 [01:26<00:00, 54.66it/s]\n"}]},{"cell_type":"markdown","source":"## Bi-directional LSTM Model\n\nBelow, I prepare text data for the machine learning model.\n\n- **Extract Engineered Features:** I extract the sentence features I engineered above and Parts of Speech counts.\n- **Fit Tokenizer:** The Tokenizer is trained on preprocessed text excerpts. This step helps the Tokenizer build a vocabulary and assign a unique integer index to each word in the text.\n- **Convert Text to Sequences:** The text sequences in the DataFrame are converted into sequences of integers using the Tokenizer\n- **Pad Sequences:** To ensure all sequences have the same length, I pad them with zeros to a maximum length.\n- **Label Creation:** Labels for the data are created from the \"BT Easiness\" column.\n- **Data Splitting:** The data is split into training and testing sets. I allocate 20% of the data for testing.\n- **Early Stopping:** I define early stopping criteria for model training.","metadata":{}},{"cell_type":"code","source":"max_sequence_length = 300\nmax_words = 50000\ndropout_rate = 0.25\n\nsentence_features = df[\n    [\n        \"mean_syllable_count\",\n        \"num_sentences\",\n        \"mean_sentence_length\",\n        \"mean_word_length\",\n    ]\n]\n\nsentence_features = sentence_features.join(\n    pd.DataFrame(df[\"pos_counts\"].tolist()).fillna(0)\n)\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(df[\"processed_excerpt\"])\nsequences = tokenizer.texts_to_sequences(df[\"processed_excerpt\"])\ndata = pad_sequences(sequences, maxlen=max_sequence_length)\nlabels = np.array(df[\"BT Easiness\"])\n\n(\n    x_text_train,\n    x_text_test,\n    x_sentence_train,\n    x_sentence_test,\n    y_train,\n    y_test,\n) = train_test_split(\n    data, sentence_features, labels, test_size=0.2, random_state=42, shuffle=True\n)\nearly_stopping = EarlyStopping(\n    monitor=\"val_loss\", patience=5, restore_best_weights=True\n)","metadata":{},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Define and train a model with text and continuous inputs\n\n- **Model Definition:** \n  - Two input layers are defined: `text_input` for text data and `continuous_input` for the engineered feature data.\n  - For the text input, \n    - I start with an Embedding layer.\n    - A Bidirectional LSTM layer processes the embedded text data.\n    - A LeakyReLU activation function is applied to the output of the LSTM layer.\n    - A Dropout layer is added to reduce overfitting.\n  - For the continuous engineered feature data, \n    - A BatchNormalization layer is applied, \n    - Then a Dense layer\n    - ReLU activation\n    - A Dropout layer\n  - Both the processed text and continuous data are concatenated.\n  - A Dense layer\n  - The final output layer is a Dense layer with 1 unit and a linear activation function for regression.\n- **Model Training:** \n  - The model is trained for 30 epochs with a batch size of 15 and early stopping.","metadata":{}},{"cell_type":"code","source":"max_sequence_length = 300\nmax_words = 50000\n\n\ntext_input = Input(name=\"text\", shape=(max_sequence_length,))\ntext_embedding = Embedding(max_words, 128, input_length=max_sequence_length)(text_input)\ntext_lstm = Bidirectional(LSTM(64))(text_embedding)\ntext_leakyrelu = LeakyReLU(alpha=0.1)(text_lstm)\ntext_dropout = Dropout(0.25)(text_leakyrelu)\n\ncontinuous_input = Input(name=\"sentence\", shape=(48,))\ncontinuous_input_bn = BatchNormalization(name=\"continuous_batch_norm\")(continuous_input)\ncontinuous_input_dense = Dense(32, activation=\"relu\", name=\"continuous_dense\")(\n    continuous_input_bn\n)\ncontinuous_input_dropout = Dropout(0.2, name=\"continuous_dropout\")(\n    continuous_input_dense\n)\n\nmerged = concatenate([text_dropout, continuous_input_dropout])\nmerged = Dense(64, activation=\"relu\")(merged)\noutput = Dense(1, activation=\"linear\")(merged)\n\nmodel = Model(inputs=[text_input, continuous_input], outputs=output)\n\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\nmodel.summary()\n\nhistory = model.fit(\n    {\"text\": x_text_train, \"sentence\": x_sentence_train},\n    y_train,\n    epochs=30,\n    batch_size=15,\n    validation_data=({\"text\": x_text_test, \"sentence\": x_sentence_test}, y_test),\n    callbacks=[early_stopping],\n)","metadata":{},"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":"Model: \"model\"\n\n__________________________________________________________________________________________________\n\n Layer (type)                Output Shape                 Param #   Connected to                  \n\n==================================================================================================\n\n text (InputLayer)           [(None, 300)]                0         []                            \n\n                                                                                                  \n\n embedding (Embedding)       (None, 300, 128)             6400000   ['text[0][0]']                \n\n                                                                                                  \n\n sentence (InputLayer)       [(None, 48)]                 0         []                            \n\n                                                                                                  \n\n bidirectional (Bidirection  (None, 128)                  98816     ['embedding[0][0]']           \n\n al)                                                                                              \n\n                                                                                                  \n\n continuous_batch_norm (Bat  (None, 48)                   192       ['sentence[0][0]']            \n\n chNormalization)                                                                                 \n\n                                                                                                  \n\n leaky_re_lu (LeakyReLU)     (None, 128)                  0         ['bidirectional[0][0]']       \n\n                                                                                                  \n\n continuous_dense (Dense)    (None, 32)                   1568      ['continuous_batch_norm[0][0]'\n\n                                                                    ]                             \n\n                                                                                                  \n\n dropout (Dropout)           (None, 128)                  0         ['leaky_re_lu[0][0]']         \n\n                                                                                                  \n\n continuous_dropout (Dropou  (None, 32)                   0         ['continuous_dense[0][0]']    \n\n t)                                                                                               \n\n                                                                                                  \n\n concatenate (Concatenate)   (None, 160)                  0         ['dropout[0][0]',             \n\n                                                                     'continuous_dropout[0][0]']  \n\n                                                                                                  \n\n dense (Dense)               (None, 64)                   10304     ['concatenate[0][0]']         \n\n                                                                                                  \n\n dense_1 (Dense)             (None, 1)                    65        ['dense[0][0]']               \n\n                                                                                                  \n\n==================================================================================================\n\nTotal params: 6510945 (24.84 MB)\n\nTrainable params: 6510849 (24.84 MB)\n\nNon-trainable params: 96 (384.00 Byte)\n\n__________________________________________________________________________________________________\n\nEpoch 1/30\n\n252/252 [==============================] - 108s 413ms/step - loss: 0.8608 - val_loss: 0.8965\n\nEpoch 2/30\n\n252/252 [==============================] - 109s 431ms/step - loss: 0.4672 - val_loss: 0.6494\n\nEpoch 3/30\n\n237/252 [===========================>..] - ETA: 6s - loss: 0.2620"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\focus\\Develop\\CU\\DTSA-5511\\final\\LSTM with Sentence and POS Features.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/focus/Develop/CU/DTSA-5511/final/LSTM%20with%20Sentence%20and%20POS%20Features.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m\"\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madam\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/focus/Develop/CU/DTSA-5511/final/LSTM%20with%20Sentence%20and%20POS%20Features.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/focus/Develop/CU/DTSA-5511/final/LSTM%20with%20Sentence%20and%20POS%20Features.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/focus/Develop/CU/DTSA-5511/final/LSTM%20with%20Sentence%20and%20POS%20Features.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     {\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m: x_text_train, \u001b[39m\"\u001b[39;49m\u001b[39msentence\u001b[39;49m\u001b[39m\"\u001b[39;49m: x_sentence_train},\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/focus/Develop/CU/DTSA-5511/final/LSTM%20with%20Sentence%20and%20POS%20Features.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     y_train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/focus/Develop/CU/DTSA-5511/final/LSTM%20with%20Sentence%20and%20POS%20Features.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/focus/Develop/CU/DTSA-5511/final/LSTM%20with%20Sentence%20and%20POS%20Features.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/focus/Develop/CU/DTSA-5511/final/LSTM%20with%20Sentence%20and%20POS%20Features.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m: x_text_test, \u001b[39m\"\u001b[39;49m\u001b[39msentence\u001b[39;49m\u001b[39m\"\u001b[39;49m: x_sentence_test}, y_test),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/focus/Develop/CU/DTSA-5511/final/LSTM%20with%20Sentence%20and%20POS%20Features.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[early_stopping],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/focus/Develop/CU/DTSA-5511/final/LSTM%20with%20Sentence%20and%20POS%20Features.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m )\n","File \u001b[1;32mc:\\Users\\focus\\Develop\\CU\\DTSA-5511\\final\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[1;32mc:\\Users\\focus\\Develop\\CU\\DTSA-5511\\final\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n","File \u001b[1;32mc:\\Users\\focus\\Develop\\CU\\DTSA-5511\\final\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[1;32mc:\\Users\\focus\\Develop\\CU\\DTSA-5511\\final\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n","File \u001b[1;32mc:\\Users\\focus\\Develop\\CU\\DTSA-5511\\final\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[0;32m    869\u001b[0m   )\n\u001b[0;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n","File \u001b[1;32mc:\\Users\\focus\\Develop\\CU\\DTSA-5511\\final\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n","File \u001b[1;32mc:\\Users\\focus\\Develop\\CU\\DTSA-5511\\final\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mflat_call(args)\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n","File \u001b[1;32mc:\\Users\\focus\\Develop\\CU\\DTSA-5511\\final\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n","File \u001b[1;32mc:\\Users\\focus\\Develop\\CU\\DTSA-5511\\final\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    253\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    254\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    255\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    256\u001b[0m     )\n\u001b[0;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\focus\\Develop\\CU\\DTSA-5511\\final\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1480\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1485\u001b[0m   )\n\u001b[0;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n","File \u001b[1;32mc:\\Users\\focus\\Develop\\CU\\DTSA-5511\\final\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":"best_epoch = np.argmin(history.history['val_loss'])\nbest_loss = history.history['val_loss'][best_epoch]\nprint(f\"The loss of the best model is: {best_loss}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the testing and training loss.","metadata":{}},{"cell_type":"code","source":"train_loss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\n\nepochs = list(range(1, len(train_loss) + 1))  # Convert range to list\n\nfig = go.Figure()\nfig.add_trace(\n    go.Scatter(\n        x=epochs,\n        y=train_loss,\n        mode=\"lines\",\n        name=\"Training Loss\",\n        line=dict(color=\"blue\"),\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=epochs,\n        y=val_loss,\n        mode=\"lines\",\n        name=\"Validation Loss\",\n        line=dict(color=\"red\"),\n    )\n)\nfig.update_layout(\n    title=\"Training and Validation Loss\",\n    xaxis_title=\"Epochs\",\n    yaxis_title=\"Loss\",\n    showlegend=True,\n    template=\"plotly_white\",\n)\nfig.show()","metadata":{},"execution_count":null,"outputs":[]}]}