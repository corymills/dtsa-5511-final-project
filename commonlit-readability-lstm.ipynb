{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install syllapy\n!pip install ety","metadata":{"execution":{"iopub.status.busy":"2023-10-15T01:31:11.812446Z","iopub.execute_input":"2023-10-15T01:31:11.812877Z","iopub.status.idle":"2023-10-15T01:31:33.382381Z","shell.execute_reply.started":"2023-10-15T01:31:11.812849Z","shell.execute_reply":"2023-10-15T01:31:33.380752Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Requirement already satisfied: syllapy in /opt/conda/lib/python3.10/site-packages (0.7.2)\nRequirement already satisfied: ety in /opt/conda/lib/python3.10/site-packages (1.4.0)\nRequirement already satisfied: treelib in /opt/conda/lib/python3.10/site-packages (from ety) (1.7.0)\nRequirement already satisfied: colorful in /opt/conda/lib/python3.10/site-packages (from ety) (0.5.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from ety) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport nltk\nfrom collections import Counter\nimport syllapy\nfrom nltk.corpus import wordnet\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport ety\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nfrom transformers import (\n    BertTokenizer,\n    TFBertModel,\n    XLMRobertaTokenizer,\n    TFXLMRobertaModel,\n)\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import (\n    Embedding,\n    Bidirectional,\n    LSTM,\n    Dense,\n    Dropout,\n    Input,\n    Flatten,\n    concatenate,\n    BatchNormalization,\n)\nfrom keras import regularizers\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model\n\ntqdm.pandas()\nnltk.download(\"wordnet\")\nnltk.download(\"punkt\")\nnltk.download(\"averaged_perceptron_tagger\")","metadata":{"execution":{"iopub.status.busy":"2023-10-15T01:31:33.385136Z","iopub.execute_input":"2023-10-15T01:31:33.385545Z","iopub.status.idle":"2023-10-15T01:31:38.300257Z","shell.execute_reply.started":"2023-10-15T01:31:33.385508Z","shell.execute_reply":"2023-10-15T01:31:38.298747Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"## Read and Preprocess Data","metadata":{}},{"cell_type":"markdown","source":"Below, I pre-process the text data along with performing feature extraction.  The following code takes text input, performs various operations on it, and returns a set of linguistic statistics and processed data.\n\nThe following code:\n\n- Converts the input text to lowercase\n- Tokenizes the text into sentences using the Natural Language Toolkit (`nltk`), and each sentence is further tokenized into words. POS tags are assigned to each word.\n- For each word in the text, the code does the following:\n  - Determines the word's origin language (etymology).\n  - Retrieves the full name of the POS tag from the `pos_mapping`.\n  - Counts the number of syllables in the word.\n  - Calculates the length of the word.\n- Calculates various linguistic statistics, such as the mean syllable count, the number of sentences, the mean sentence length, the mean word length, and the total number of words.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\n    \"/kaggle/input/clear-corpus-6-01-clear-corpus-6-01/CLEAR Corpus 6.01 - CLEAR Corpus 6.01.csv\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T01:31:38.302246Z","iopub.execute_input":"2023-10-15T01:31:38.303380Z","iopub.status.idle":"2023-10-15T01:31:38.479380Z","shell.execute_reply.started":"2023-10-15T01:31:38.303344Z","shell.execute_reply":"2023-10-15T01:31:38.477869Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Define a mapping of POS (Part of Speech) tags to their full names\npos_mapping = {\n    \"CC\": \"Coordinating Conjunction\",\n    \"CD\": \"Cardinal Digit\",\n    \"DT\": \"Determiner\",\n    \"EX\": \"Existential There\",\n    \"FW\": \"Foreign Word\",\n    \"IN\": \"Preposition or Subordinating Conjunction\",\n    \"JJ\": \"Adjective\",\n    \"JJR\": \"Adjective, Comparative\",\n    \"JJS\": \"Adjective, Superlative\",\n    \"LS\": \"List Item Marker\",\n    \"MD\": \"Modal\",\n    \"NN\": \"Noun, Singular or Mass\",\n    \"NNS\": \"Noun, Plural\",\n    \"NNP\": \"Proper Noun, Singular\",\n    \"NNPS\": \"Proper Noun, Plural\",\n    \"PDT\": \"Predeterminer\",\n    \"POS\": \"Possessive Ending\",\n    \"PRP\": \"Personal Pronoun\",\n    \"PRP$\": \"Possessive Pronoun\",\n    \"RB\": \"Adverb\",\n    \"RBR\": \"Adverb, Comparative\",\n    \"RBS\": \"Adverb, Superlative\",\n    \"RP\": \"Particle\",\n    \"TO\": \"to\",\n    \"UH\": \"Interjection\",\n    \"VB\": \"Verb, Base Form\",\n    \"VBD\": \"Verb, Past Tense\",\n    \"VBG\": \"Verb, Gerund or Present Participle\",\n    \"VBN\": \"Verb, Past Participle\",\n    \"VBP\": \"Verb, Non-3rd Person Singular Present\",\n    \"VBZ\": \"Verb, 3rd Person Singular Present\",\n    \"WDT\": \"Wh-determiner\",\n    \"WP\": \"Wh-pronoun\",\n    \"WP$\": \"Possessive Wh-pronoun\",\n    \"WRB\": \"Wh-adverb\",\n}\n\n\n# Define a function to process text\ndef process_text(text):\n    # Convert text to lowercase\n    text = text.lower()\n\n    # Initialize lists to store various linguistic features\n    word_origins = []\n    word_pos = []\n    syllable_counts = []\n    sentence_lengths = []\n    word_lengths = []\n\n    # Tokenize the text into sentences\n    sentences = sent_tokenize(text)\n\n    # Iterate through each sentence\n    for sentence in sentences:\n        # Tokenize each sentence into words and determine POS tags\n        tokens = nltk.word_tokenize(sentence)\n        pos_tags = nltk.pos_tag(tokens)\n        sentence_lengths.append(len(pos_tags))\n\n        # Process each word in the sentence\n        for token, pos in pos_tags:\n            # Determine the word's origin language (etymology)\n            origin = ety.origins(token)\n            if origin:\n                origin = origin[0].language.name\n            else:\n                origin = \"unknown\"\n            word_origins.append(origin)\n\n            # Get the full name of the POS tag using the mapping\n            full_pos_name = pos_mapping.get(pos, pos)\n            word_pos.append(full_pos_name)\n\n            # Count the number of syllables in the word\n            syllables = syllapy.count(token)\n            syllable_counts.append(syllables)\n\n            # Calculate the length of the word\n            word_lengths.append(len(token))\n\n    # Calculate various linguistic statistics\n    processed_excerpt = text\n    origin_counts = Counter(word_origins)\n    pos_counts = Counter(word_pos)\n    mean_syllable_count = np.mean(syllable_counts)\n    num_sentences = len(sentences)\n    mean_sentence_length = np.mean(sentence_lengths)\n    num_words = np.sum(sentence_lengths)\n    mean_word_length = np.mean(word_lengths)\n\n    # Return the processed data and statistics\n    return (\n        word_origins,\n        origin_counts,\n        word_pos,\n        pos_counts,\n        syllable_counts,\n        mean_syllable_count,\n        num_sentences,\n        mean_sentence_length,\n        mean_word_length,\n        num_words,\n        processed_excerpt,\n    )","metadata":{"execution":{"iopub.status.busy":"2023-10-15T01:31:38.484203Z","iopub.execute_input":"2023-10-15T01:31:38.484591Z","iopub.status.idle":"2023-10-15T01:31:38.499879Z","shell.execute_reply.started":"2023-10-15T01:31:38.484563Z","shell.execute_reply":"2023-10-15T01:31:38.498070Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df[\n    [\n        \"word_origins\",\n        \"word_origin_counts\",\n        \"pos\",\n        \"pos_counts\",\n        \"syllable_counts\",\n        \"mean_syllable_count\",\n        \"num_sentences\",\n        \"mean_sentence_length\",\n        \"mean_word_length\",\n        \"num_words\",\n        \"processed_excerpt\",\n    ]\n] = df[\"Excerpt\"].progress_apply(lambda x: pd.Series(process_text(x)))","metadata":{"execution":{"iopub.status.busy":"2023-10-15T01:31:38.502516Z","iopub.execute_input":"2023-10-15T01:31:38.502884Z","iopub.status.idle":"2023-10-15T01:32:49.401517Z","shell.execute_reply.started":"2023-10-15T01:31:38.502836Z","shell.execute_reply":"2023-10-15T01:32:49.400130Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"100%|██████████| 4724/4724 [01:10<00:00, 66.66it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Bi-directional LSTM Model\n\nBelow, I prepare text data for the machine learning model.\n\n- **Fit Tokenizer:** The Tokenizer is trained on preprocessed text excerpts. This step helps the Tokenizer build a vocabulary and assign a unique integer index to each word in the text.\n- **Convert Text to Sequences:** The text sequences in the DataFrame are converted into sequences of integers using the Tokenizer\n- **Pad Sequences:** To ensure all sequences have the same length, I pad them with zeros to a maximum length.\n- **Label Creation:** Labels for the data are created from the \"BT Easiness\" column.\n- **Data Splitting:** The data is split into training and testing sets. I allocate 20% of the data for testing.\n- **Early Stopping:** I define early stopping criteria for model training.","metadata":{}},{"cell_type":"code","source":"# Set the maximum sequence length for text data\nmax_sequence_length = 300\n\n# Set the maximum number of words to consider in the Tokenizer\nmax_words = 50000\n\n# Initialize a Tokenizer with a specified maximum number of words\ntokenizer = Tokenizer(num_words=max_words)\n\n# Fit the Tokenizer on the preprocessed excerpts in the DataFrame\ntokenizer.fit_on_texts(df[\"processed_excerpt\"])\n\n# Convert text sequences into sequences of integers using the Tokenizer\nsequences = tokenizer.texts_to_sequences(df[\"processed_excerpt\"])\n\n# Pad the sequences to ensure uniform length\ndata = pad_sequences(sequences, maxlen=max_sequence_length)\n\n# Create an array of labels based on the \"BT Easiness\" column in the DataFrame\nlabels = np.array(df[\"BT Easiness\"])\n\n# Split the data into training and testing sets\n(\n    x_text_train,\n    x_text_test,\n    y_train,\n    y_test,\n) = train_test_split(data, labels, test_size=0.2, random_state=42, shuffle=True)\n\n# Define early stopping criteria for model training\nearly_stopping = EarlyStopping(\n    monitor=\"val_loss\", patience=5, restore_best_weights=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T01:32:49.402687Z","iopub.execute_input":"2023-10-15T01:32:49.403019Z","iopub.status.idle":"2023-10-15T01:32:50.928806Z","shell.execute_reply.started":"2023-10-15T01:32:49.402993Z","shell.execute_reply":"2023-10-15T01:32:50.927642Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Define and train the model\n\n- **Model Definition:** \n  - A Sequential model is created.\n  - The first layer is an Embedding layer, which is used for text data.\n  - A Bidirectional LSTM layer with 64 units follows the Embedding layer. Bidirectional LSTM processes sequences in both forward and backward directions, capturing context effectively.\n  - A LeakyReLU activation function with an alpha of 0.1 is applied to the output of the LSTM layer.\n  - A Dropout layer with a rate of 0.25 is added to reduce overfitting.\n  - Finally, a Dense layer with one unit and a linear activation function is used for regression.\n- **Model Training:** \n  - The model is trained for 30 epochs with a batch size of 50 and early stopping.","metadata":{}},{"cell_type":"code","source":"# Create a Sequential model for neural network architecture\nmodel = Sequential()\nmodel.add(Embedding(max_words, 128, input_length=max_sequence_length))\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(1, activation=\"linear\"))\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n\n# Display a summary of the model architecture\nmodel.summary()\n\n# Train the model\nhistory = model.fit(\n    x_text_train,\n    y_train,\n    epochs=30,\n    batch_size=50,\n    validation_data=(x_text_test, y_test),\n    callbacks=[early_stopping],\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T01:32:50.931255Z","iopub.execute_input":"2023-10-15T01:32:50.931904Z","iopub.status.idle":"2023-10-15T01:33:07.194998Z","shell.execute_reply.started":"2023-10-15T01:32:50.931846Z","shell.execute_reply":"2023-10-15T01:33:07.192678Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 300, 128)          6400000   \n                                                                 \n bidirectional (Bidirectiona  (None, 128)              98816     \n l)                                                              \n                                                                 \n leaky_re_lu (LeakyReLU)     (None, 128)               0         \n                                                                 \n dropout (Dropout)           (None, 128)               0         \n                                                                 \n dense (Dense)               (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 6,498,945\nTrainable params: 6,498,945\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/30\n20/76 [======>.......................] - ETA: 26s - loss: 1.5243","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_text_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_text_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"best_epoch = np.argmin(history.history['val_loss'])\nbest_loss = history.history['val_loss'][best_epoch]\nprint(f\"The loss of the best model is: {best_loss}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the testing and training loss.","metadata":{}},{"cell_type":"code","source":"# Extract training loss and validation loss from the history object\ntrain_loss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\n\n# Create a list of epochs for the x-axis, converting range to list\nepochs = list(range(1, len(train_loss) + 1))\n\n# Initialize a Plotly Figure for the line chart\nfig = go.Figure()\n\n# Add a trace for training loss as a blue line\nfig.add_trace(\n    go.Scatter(\n        x=epochs,\n        y=train_loss,\n        mode=\"lines\",\n        name=\"Training Loss\",\n        line=dict(color=\"blue\"),\n    )\n)\n\n# Add a trace for validation loss as a red line\nfig.add_trace(\n    go.Scatter(\n        x=epochs,\n        y=val_loss,\n        mode=\"lines\",\n        name=\"Validation Loss\",\n        line=dict(color=\"red\"),\n    )\n)\n\n# Update the layout of the figure with title and axis labels\nfig.update_layout(\n    title=\"Training and Validation Loss\",\n    xaxis_title=\"Epochs\",\n    yaxis_title=\"Loss\",\n    showlegend=True,\n    template=\"plotly_white\",\n)\n\n# Display the figure\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-15T01:33:07.196705Z","iopub.status.idle":"2023-10-15T01:33:07.197288Z","shell.execute_reply.started":"2023-10-15T01:33:07.196987Z","shell.execute_reply":"2023-10-15T01:33:07.197011Z"},"trusted":true},"execution_count":null,"outputs":[]}]}