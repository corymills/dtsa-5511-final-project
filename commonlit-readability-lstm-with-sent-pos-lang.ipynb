{"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install syllapy\n!pip install ety","metadata":{},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: syllapy in c:\\users\\focus\\develop\\cu\\dtsa-5511\\final\\.venv\\lib\\site-packages (0.7.2)\n\nRequirement already satisfied: ety in c:\\users\\focus\\develop\\cu\\dtsa-5511\\final\\.venv\\lib\\site-packages (1.4.0)\n\nRequirement already satisfied: treelib in c:\\users\\focus\\develop\\cu\\dtsa-5511\\final\\.venv\\lib\\site-packages (from ety) (1.7.0)\n\nRequirement already satisfied: colorful in c:\\users\\focus\\develop\\cu\\dtsa-5511\\final\\.venv\\lib\\site-packages (from ety) (0.5.5)\n\nRequirement already satisfied: six in c:\\users\\focus\\develop\\cu\\dtsa-5511\\final\\.venv\\lib\\site-packages (from ety) (1.16.0)\n\nRequirement already satisfied: colorama in c:\\users\\focus\\develop\\cu\\dtsa-5511\\final\\.venv\\lib\\site-packages (from colorful->ety) (0.4.6)\n"}]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport nltk\nfrom collections import Counter\nimport syllapy\nfrom nltk.corpus import wordnet\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport ety\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nfrom transformers import (\n    BertTokenizer,\n    TFBertModel,\n    XLMRobertaTokenizer,\n    TFXLMRobertaModel,\n)\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import (\n    Embedding,\n    Bidirectional,\n    LSTM,\n    Dense,\n    Dropout,\n    Input,\n    Flatten,\n    concatenate,\n    BatchNormalization,\n)\nfrom keras import regularizers\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model\n\ntqdm.pandas()\nnltk.download(\"wordnet\")\nnltk.download(\"punkt\")\nnltk.download(\"averaged_perceptron_tagger\")","metadata":{},"execution_count":2,"outputs":[{"name":"stderr","output_type":"stream","text":"c:\\Users\\focus\\Develop\\CU\\DTSA-5511\\final\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n\n  from .autonotebook import tqdm as notebook_tqdm\n\n[nltk_data] Downloading package wordnet to\n\n[nltk_data]     C:\\Users\\focus\\AppData\\Roaming\\nltk_data...\n\n[nltk_data]   Package wordnet is already up-to-date!\n\n[nltk_data] Downloading package punkt to\n\n[nltk_data]     C:\\Users\\focus\\AppData\\Roaming\\nltk_data...\n\n[nltk_data]   Package punkt is already up-to-date!\n\n[nltk_data] Downloading package averaged_perceptron_tagger to\n\n[nltk_data]     C:\\Users\\focus\\AppData\\Roaming\\nltk_data...\n\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n\n[nltk_data]       date!\n"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{}}]},{"cell_type":"markdown","source":"## Read and Process Data","metadata":{}},{"cell_type":"markdown","source":"Below, I pre-process the text data along with performing feature extraction.  The following code takes text input, performs various operations on it, and returns a set of linguistic statistics and processed data.\n\nThe following code:\n\n- Converts the input text to lowercase\n- Tokenizes the text into sentences using the Natural Language Toolkit (`nltk`), and each sentence is further tokenized into words. POS tags are assigned to each word.\n- For each word in the text, the code does the following:\n  - Determines the word's origin language (etymology).\n  - Retrieves the full name of the POS tag from the `pos_mapping`.\n  - Counts the number of syllables in the word.\n  - Calculates the length of the word.\n- Calculates various linguistic statistics, such as the mean syllable count, the number of sentences, the mean sentence length, the mean word length, and the total number of words.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\n    \"/kaggle/input/clear-corpus-6-01-clear-corpus-6-01/CLEAR Corpus 6.01 - CLEAR Corpus 6.01.csv\"\n)","metadata":{},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"pos_mapping = {\n    \"CC\": \"Coordinating Conjunction\",\n    \"CD\": \"Cardinal Digit\",\n    \"DT\": \"Determiner\",\n    \"EX\": \"Existential There\",\n    \"FW\": \"Foreign Word\",\n    \"IN\": \"Preposition or Subordinating Conjunction\",\n    \"JJ\": \"Adjective\",\n    \"JJR\": \"Adjective, Comparative\",\n    \"JJS\": \"Adjective, Superlative\",\n    \"LS\": \"List Item Marker\",\n    \"MD\": \"Modal\",\n    \"NN\": \"Noun, Singular or Mass\",\n    \"NNS\": \"Noun, Plural\",\n    \"NNP\": \"Proper Noun, Singular\",\n    \"NNPS\": \"Proper Noun, Plural\",\n    \"PDT\": \"Predeterminer\",\n    \"POS\": \"Possessive Ending\",\n    \"PRP\": \"Personal Pronoun\",\n    \"PRP$\": \"Possessive Pronoun\",\n    \"RB\": \"Adverb\",\n    \"RBR\": \"Adverb, Comparative\",\n    \"RBS\": \"Adverb, Superlative\",\n    \"RP\": \"Particle\",\n    \"TO\": \"to\",\n    \"UH\": \"Interjection\",\n    \"VB\": \"Verb, Base Form\",\n    \"VBD\": \"Verb, Past Tense\",\n    \"VBG\": \"Verb, Gerund or Present Participle\",\n    \"VBN\": \"Verb, Past Participle\",\n    \"VBP\": \"Verb, Non-3rd Person Singular Present\",\n    \"VBZ\": \"Verb, 3rd Person Singular Present\",\n    \"WDT\": \"Wh-determiner\",\n    \"WP\": \"Wh-pronoun\",\n    \"WP$\": \"Possessive Wh-pronoun\",\n    \"WRB\": \"Wh-adverb\",\n}\n\n\ndef process_text(text):\n    text = text.lower()\n\n    word_origins = []\n    word_pos = []\n    syllable_counts = []\n    sentence_lengths = []\n    word_lengths = []\n\n    sentences = sent_tokenize(text)\n\n    for sentence in sentences:\n        tokens = nltk.word_tokenize(sentence)\n        pos_tags = nltk.pos_tag(tokens)\n        sentence_lengths.append(len(pos_tags))\n        for token, pos in pos_tags:\n            origin = ety.origins(token)\n            if origin:\n                origin = origin[0].language.name\n            else:\n                origin = \"unknown\"\n            word_origins.append(origin)\n            full_pos_name = pos_mapping.get(pos, pos)\n            word_pos.append(full_pos_name)\n            syllables = syllapy.count(token)\n            syllable_counts.append(syllables)\n            word_lengths.append(len(token))\n\n    processed_excerpt = text\n    origin_counts = Counter(word_origins)\n    pos_counts = Counter(word_pos)\n    mean_syllable_count = np.mean(syllable_counts)\n    num_sentences = len(sentences)\n    mean_sentence_length = np.mean(sentence_lengths)\n    num_words = np.sum(sentence_lengths)\n    mean_word_length = np.mean(word_lengths)\n\n    return (\n        word_origins,\n        origin_counts,\n        word_pos,\n        pos_counts,\n        syllable_counts,\n        mean_syllable_count,\n        num_sentences,\n        mean_sentence_length,\n        mean_word_length,\n        num_words,\n        processed_excerpt,\n    )","metadata":{},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df[\n    [\n        \"word_origins\",\n        \"word_origin_counts\",\n        \"pos\",\n        \"pos_counts\",\n        \"syllable_counts\",\n        \"mean_syllable_count\",\n        \"num_sentences\",\n        \"mean_sentence_length\",\n        \"mean_word_length\",\n        \"num_words\",\n        \"processed_excerpt\",\n    ]\n] = df[\"Excerpt\"].progress_apply(lambda x: pd.Series(process_text(x)))","metadata":{},"execution_count":5,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 4724/4724 [05:39<00:00, 13.92it/s]\n"}]},{"cell_type":"markdown","source":"## Bi-directional LSTM Model\n\nBelow, I prepare text data for the machine learning model.\n\n- **Extract Engineered Features:** I extract the sentence features, Parts of Speech counts, and language origin counts engineered above.\n- **Fit Tokenizer:** The Tokenizer is trained on preprocessed text excerpts. This step helps the Tokenizer build a vocabulary and assign a unique integer index to each word in the text.\n- **Convert Text to Sequences:** The text sequences in the DataFrame are converted into sequences of integers using the Tokenizer\n- **Pad Sequences:** To ensure all sequences have the same length, I pad them with zeros to a maximum length.\n- **Label Creation:** Labels for the data are created from the \"BT Easiness\" column.\n- **Data Splitting:** The data is split into training and testing sets. I allocate 20% of the data for testing.\n- **Early Stopping:** I define early stopping criteria for model training.","metadata":{}},{"cell_type":"code","source":"max_sequence_length = 300\nmax_words = 50000\ndropout_rate = 0.25\n\nsentence_features = df[\n    [\n        \"mean_syllable_count\",\n        \"num_sentences\",\n        \"mean_sentence_length\",\n        \"mean_word_length\",\n    ]\n]\n\nsentence_features = sentence_features.join(\n    pd.DataFrame(df[\"pos_counts\"].tolist()).fillna(0)\n)\n\nsentence_features = sentence_features.join(\n    pd.DataFrame(df[\"word_origin_counts\"].tolist()).fillna(0)\n)\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(df[\"processed_excerpt\"])\nsequences = tokenizer.texts_to_sequences(df[\"processed_excerpt\"])\ndata = pad_sequences(sequences, maxlen=max_sequence_length)\nlabels = np.array(df[\"BT Easiness\"])\n\n(\n    x_text_train,\n    x_text_test,\n    x_sentence_train,\n    x_sentence_test,\n    y_train,\n    y_test,\n) = train_test_split(\n    data, sentence_features, labels, test_size=0.2, random_state=42, shuffle=True\n)\nearly_stopping = EarlyStopping(\n    monitor=\"val_loss\", patience=5, restore_best_weights=True\n)","metadata":{},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Define and train a model with text and continuous inputs\n\n- **Model Definition:** \n  - Two input layers are defined: `text_input` for text data and `continuous_input` for the engineered feature data.\n  - For the text input, \n    - I start with an Embedding layer.\n    - A Bidirectional LSTM layer processes the embedded text data.\n    - A LeakyReLU activation function is applied to the output of the LSTM layer.\n    - A Dropout layer is added to reduce overfitting.\n  - For the continuous engineered feature data, \n    - A BatchNormalization layer is applied, \n    - Then a Dense layer\n    - ReLU activation\n    - A Dropout layer\n  - Both the processed text and continuous data are concatenated.\n  - A Dense layer\n  - The final output layer is a Dense layer with 1 unit and a linear activation function for regression.\n- **Model Training:** \n  - The model is trained for 30 epochs with a batch size of 15 and early stopping.","metadata":{}},{"cell_type":"code","source":"max_sequence_length = 300\nmax_words = 50000\n\n\ntext_input = Input(name=\"text\", shape=(max_sequence_length,))\ntext_embedding = Embedding(max_words, 128, input_length=max_sequence_length)(text_input)\ntext_lstm = Bidirectional(LSTM(64))(text_embedding)\ntext_leakyrelu = LeakyReLU(alpha=0.1)(text_lstm)\ntext_dropout = Dropout(0.25)(text_leakyrelu)\n\ncontinuous_input = Input(name=\"sentence\", shape=(118,))\ncontinuous_input_bn = BatchNormalization(name=\"continuous_batch_norm\")(continuous_input)\ncontinuous_input_dense = Dense(32, activation=\"relu\", name=\"continuous_dense\")(\n    continuous_input_bn\n)\ncontinuous_input_dropout = Dropout(0.2, name=\"continuous_dropout\")(\n    continuous_input_dense\n)\n\nmerged = concatenate([text_dropout, continuous_input_dropout])\nmerged = Dense(64, activation=\"relu\")(merged)\noutput = Dense(1, activation=\"linear\")(merged)\n\nmodel = Model(inputs=[text_input, continuous_input], outputs=output)\n\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\nmodel.summary()\n\nhistory = model.fit(\n    {\"text\": x_text_train, \"sentence\": x_sentence_train},\n    y_train,\n    epochs=30,\n    batch_size=15,\n    validation_data=({\"text\": x_text_test, \"sentence\": x_sentence_test}, y_test),\n    callbacks=[early_stopping],\n)","metadata":{},"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":"Model: \"model\"\n\n__________________________________________________________________________________________________\n\n Layer (type)                Output Shape                 Param #   Connected to                  \n\n==================================================================================================\n\n text (InputLayer)           [(None, 300)]                0         []                            \n\n                                                                                                  \n\n embedding (Embedding)       (None, 300, 128)             6400000   ['text[0][0]']                \n\n                                                                                                  \n\n sentence (InputLayer)       [(None, 118)]                0         []                            \n\n                                                                                                  \n\n bidirectional (Bidirection  (None, 128)                  98816     ['embedding[0][0]']           \n\n al)                                                                                              \n\n                                                                                                  \n\n continuous_batch_norm (Bat  (None, 118)                  472       ['sentence[0][0]']            \n\n chNormalization)                                                                                 \n\n                                                                                                  \n\n leaky_re_lu (LeakyReLU)     (None, 128)                  0         ['bidirectional[0][0]']       \n\n                                                                                                  \n\n continuous_dense (Dense)    (None, 32)                   3808      ['continuous_batch_norm[0][0]'\n\n                                                                    ]                             \n\n                                                                                                  \n\n dropout (Dropout)           (None, 128)                  0         ['leaky_re_lu[0][0]']         \n\n                                                                                                  \n\n continuous_dropout (Dropou  (None, 32)                   0         ['continuous_dense[0][0]']    \n\n t)                                                                                               \n\n                                                                                                  \n\n concatenate (Concatenate)   (None, 160)                  0         ['dropout[0][0]',             \n\n                                                                     'continuous_dropout[0][0]']  \n\n                                                                                                  \n\n dense (Dense)               (None, 64)                   10304     ['concatenate[0][0]']         \n\n                                                                                                  \n\n dense_1 (Dense)             (None, 1)                    65        ['dense[0][0]']               \n\n                                                                                                  \n\n==================================================================================================\n\nTotal params: 6513465 (24.85 MB)\n\nTrainable params: 6513229 (24.85 MB)\n\nNon-trainable params: 236 (944.00 Byte)\n\n__________________________________________________________________________________________________\n\nEpoch 1/30\n\n252/252 [==============================] - 246s 807ms/step - loss: 0.7802 - val_loss: 0.7083\n\nEpoch 2/30\n\n 19/252 [=>............................] - ETA: 2:43 - loss: 0.4692"}]},{"cell_type":"code","source":"best_epoch = np.argmin(history.history['val_loss'])\nbest_loss = history.history['val_loss'][best_epoch]\nprint(f\"The loss of the best model is: {best_loss}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the testing and training loss.","metadata":{}},{"cell_type":"code","source":"train_loss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\n\nepochs = list(range(1, len(train_loss) + 1))  # Convert range to list\n\nfig = go.Figure()\nfig.add_trace(\n    go.Scatter(\n        x=epochs,\n        y=train_loss,\n        mode=\"lines\",\n        name=\"Training Loss\",\n        line=dict(color=\"blue\"),\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=epochs,\n        y=val_loss,\n        mode=\"lines\",\n        name=\"Validation Loss\",\n        line=dict(color=\"red\"),\n    )\n)\nfig.update_layout(\n    title=\"Training and Validation Loss\",\n    xaxis_title=\"Epochs\",\n    yaxis_title=\"Loss\",\n    showlegend=True,\n    template=\"plotly_white\",\n)\nfig.show()","metadata":{},"execution_count":null,"outputs":[]}]}