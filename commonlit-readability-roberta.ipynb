{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RoBERTa","metadata":{}},{"cell_type":"code","source":"!pip install syllapy\n!pip install ety","metadata":{"execution":{"iopub.execute_input":"2023-10-14T14:43:40.055083Z","iopub.status.busy":"2023-10-14T14:43:40.052185Z","iopub.status.idle":"2023-10-14T14:44:01.312033Z","shell.execute_reply":"2023-10-14T14:44:01.310334Z","shell.execute_reply.started":"2023-10-14T14:43:40.055009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport nltk\nfrom collections import Counter\nimport syllapy\nfrom nltk.corpus import wordnet\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport ety\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nfrom transformers import (\n    BertTokenizer,\n    TFBertModel,\n    XLMRobertaTokenizer,\n    TFXLMRobertaModel,\n)\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import (\n    Embedding,\n    Bidirectional,\n    LSTM,\n    Dense,\n    Dropout,\n    Input,\n    Flatten,\n    concatenate,\n    BatchNormalization,\n)\nfrom keras import regularizers\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model\n\ntqdm.pandas()\nnltk.download(\"wordnet\")\nnltk.download(\"punkt\")\nnltk.download(\"averaged_perceptron_tagger\")","metadata":{"execution":{"iopub.execute_input":"2023-10-14T14:44:01.317346Z","iopub.status.busy":"2023-10-14T14:44:01.316958Z","iopub.status.idle":"2023-10-14T14:44:01.337667Z","shell.execute_reply":"2023-10-14T14:44:01.336192Z","shell.execute_reply.started":"2023-10-14T14:44:01.317314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read and Process Data","metadata":{}},{"cell_type":"markdown","source":"Below, I pre-process the text data along with performing feature extraction.  The following code takes text input, performs various operations on it, and returns a set of linguistic statistics and processed data.\n\nThe following code:\n\n- Converts the input text to lowercase\n- Tokenizes the text into sentences using the Natural Language Toolkit (`nltk`), and each sentence is further tokenized into words. POS tags are assigned to each word.\n- For each word in the text, the code does the following:\n  - Determines the word's origin language (etymology).\n  - Retrieves the full name of the POS tag from the `pos_mapping`.\n  - Counts the number of syllables in the word.\n  - Calculates the length of the word.\n- Calculates various linguistic statistics, such as the mean syllable count, the number of sentences, the mean sentence length, the mean word length, and the total number of words.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\n    \"/kaggle/input/clear-corpus-6-01-clear-corpus-6-01/CLEAR Corpus 6.01 - CLEAR Corpus 6.01.csv\"\n)","metadata":{"execution":{"iopub.execute_input":"2023-10-14T14:44:01.339572Z","iopub.status.busy":"2023-10-14T14:44:01.339053Z","iopub.status.idle":"2023-10-14T14:44:01.529059Z","shell.execute_reply":"2023-10-14T14:44:01.527758Z","shell.execute_reply.started":"2023-10-14T14:44:01.339538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_mapping = {\n    \"CC\": \"Coordinating Conjunction\",\n    \"CD\": \"Cardinal Digit\",\n    \"DT\": \"Determiner\",\n    \"EX\": \"Existential There\",\n    \"FW\": \"Foreign Word\",\n    \"IN\": \"Preposition or Subordinating Conjunction\",\n    \"JJ\": \"Adjective\",\n    \"JJR\": \"Adjective, Comparative\",\n    \"JJS\": \"Adjective, Superlative\",\n    \"LS\": \"List Item Marker\",\n    \"MD\": \"Modal\",\n    \"NN\": \"Noun, Singular or Mass\",\n    \"NNS\": \"Noun, Plural\",\n    \"NNP\": \"Proper Noun, Singular\",\n    \"NNPS\": \"Proper Noun, Plural\",\n    \"PDT\": \"Predeterminer\",\n    \"POS\": \"Possessive Ending\",\n    \"PRP\": \"Personal Pronoun\",\n    \"PRP$\": \"Possessive Pronoun\",\n    \"RB\": \"Adverb\",\n    \"RBR\": \"Adverb, Comparative\",\n    \"RBS\": \"Adverb, Superlative\",\n    \"RP\": \"Particle\",\n    \"TO\": \"to\",\n    \"UH\": \"Interjection\",\n    \"VB\": \"Verb, Base Form\",\n    \"VBD\": \"Verb, Past Tense\",\n    \"VBG\": \"Verb, Gerund or Present Participle\",\n    \"VBN\": \"Verb, Past Participle\",\n    \"VBP\": \"Verb, Non-3rd Person Singular Present\",\n    \"VBZ\": \"Verb, 3rd Person Singular Present\",\n    \"WDT\": \"Wh-determiner\",\n    \"WP\": \"Wh-pronoun\",\n    \"WP$\": \"Possessive Wh-pronoun\",\n    \"WRB\": \"Wh-adverb\",\n}\n\n\ndef process_text(text):\n    text = text.lower()\n\n    word_origins = []\n    word_pos = []\n    syllable_counts = []\n    sentence_lengths = []\n    word_lengths = []\n\n    sentences = sent_tokenize(text)\n\n    for sentence in sentences:\n        tokens = nltk.word_tokenize(sentence)\n        pos_tags = nltk.pos_tag(tokens)\n        sentence_lengths.append(len(pos_tags))\n        for token, pos in pos_tags:\n            origin = ety.origins(token)\n            if origin:\n                origin = origin[0].language.name\n            else:\n                origin = \"unknown\"\n            word_origins.append(origin)\n            full_pos_name = pos_mapping.get(pos, pos)\n            word_pos.append(full_pos_name)\n            syllables = syllapy.count(token)\n            syllable_counts.append(syllables)\n            word_lengths.append(len(token))\n\n    processed_excerpt = text\n    origin_counts = Counter(word_origins)\n    pos_counts = Counter(word_pos)\n    mean_syllable_count = np.mean(syllable_counts)\n    num_sentences = len(sentences)\n    mean_sentence_length = np.mean(sentence_lengths)\n    num_words = np.sum(sentence_lengths)\n    mean_word_length = np.mean(word_lengths)\n\n    return (\n        word_origins,\n        origin_counts,\n        word_pos,\n        pos_counts,\n        syllable_counts,\n        mean_syllable_count,\n        num_sentences,\n        mean_sentence_length,\n        mean_word_length,\n        num_words,\n        processed_excerpt,\n    )","metadata":{"execution":{"iopub.execute_input":"2023-10-14T14:44:01.532853Z","iopub.status.busy":"2023-10-14T14:44:01.532067Z","iopub.status.idle":"2023-10-14T14:44:01.545017Z","shell.execute_reply":"2023-10-14T14:44:01.543880Z","shell.execute_reply.started":"2023-10-14T14:44:01.532818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\n    [\n        \"word_origins\",\n        \"word_origin_counts\",\n        \"pos\",\n        \"pos_counts\",\n        \"syllable_counts\",\n        \"mean_syllable_count\",\n        \"num_sentences\",\n        \"mean_sentence_length\",\n        \"mean_word_length\",\n        \"num_words\",\n        \"processed_excerpt\",\n    ]\n] = df[\"Excerpt\"].progress_apply(lambda x: pd.Series(process_text(x)))","metadata":{"execution":{"iopub.execute_input":"2023-10-14T14:44:01.547751Z","iopub.status.busy":"2023-10-14T14:44:01.546736Z","iopub.status.idle":"2023-10-14T14:44:41.346292Z","shell.execute_reply":"2023-10-14T14:44:41.345118Z","shell.execute_reply.started":"2023-10-14T14:44:01.547703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RoBERTa Model","metadata":{}},{"cell_type":"markdown","source":"Below, I prepare text data for the machine learning model.\n\n- **RoBERTa Fine Tuning:** Set the last layer of RoBERTa to be trainable, so we can fine tune it.\n- **Fit Tokenizer:** The Tokenizer is trained on preprocessed text excerpts. This step helps the Tokenizer build a vocabulary and assign a unique integer index to each word in the text.\n- **Convert Text to Sequences:** The text sequences in the DataFrame are converted into sequences of integers using the Tokenizer\n- **Pad Sequences:** To ensure all sequences have the same length, I pad them with zeros to a maximum length.\n- **Attention Masks:** Apply attention masks.\n- **Label Creation:** Labels for the data are created from the \"BT Easiness\" column.\n- **Data Splitting:** The data is split into training and testing sets. I allocate 20% of the data for testing.\n- **Early Stopping:** I define early stopping criteria for model training.","metadata":{}},{"cell_type":"code","source":"max_sequence_length = 400\nlabels = df[\"BT Easiness\"]\n\nxlm_roberta_model_name = \"xlm-roberta-base\"\ntokenizer = XLMRobertaTokenizer.from_pretrained(xlm_roberta_model_name)\nxlm_roberta_model = TFXLMRobertaModel.from_pretrained(xlm_roberta_model_name)\n\n# Set XLM-RoBERTa layers to non-trainable except the last one\nfor layer in xlm_roberta_model.layers:\n    layer.trainable = False\nfor layer in xlm_roberta_model.layers[-1:]:\n    layer.trainable = True\n\n# Tokenization and data preparation using XLM-RoBERTa\ninput_ids = []\nattention_mask = []\nfor text in df[\"processed_excerpt\"]:\n    tokens = tokenizer(\n        text,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_sequence_length,\n        return_tensors=\"tf\",\n    )\n    input_ids.append(tokens[\"input_ids\"][0])\n    attention_mask.append(tokens[\"attention_mask\"][0])\n\n# Split the data into training and testing sets\n(\n    input_ids_train,\n    input_ids_test,\n    attention_mask_train,\n    attention_mask_test,\n    labels_train,\n    labels_test,\n) = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n\n# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n    (\n        {\n            \"input_ids\": input_ids_train,\n            \"attention_mask\": attention_mask_train,\n        },\n        labels_train,\n    )\n).batch(2)\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n    (\n        {\n            \"input_ids\": input_ids_test,\n            \"attention_mask\": attention_mask_test,\n        },\n        labels_test,\n    )\n).batch(2)","metadata":{"execution":{"iopub.execute_input":"2023-10-14T14:44:41.348429Z","iopub.status.busy":"2023-10-14T14:44:41.347968Z","iopub.status.idle":"2023-10-14T14:44:59.557911Z","shell.execute_reply":"2023-10-14T14:44:59.557125Z","shell.execute_reply.started":"2023-10-14T14:44:41.348398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define and train a model with text inputs\n\n- **Model Definition**\n    - Two input layers are defined: `input_ids_layer` and `attention_mask_layer` for processing text data.\n    - The XLM-RoBERTa model is used to process the input text data.\n    - The output is flattened.\n    - A dropout layer is applied to reduce overfitting.\n    - Batch Normalization is applied to the dropout output.\n    - A dense layer follows\n    - Followed by the output layer for regression.\n- **Model Training:**\n    - The model is trained for 30 epochs with a batch size of 15 and early stopping.","metadata":{}},{"cell_type":"code","source":"early_stopping = EarlyStopping(\n    monitor=\"val_loss\", patience=5, restore_best_weights=True\n)\n\n# Define input layers\ninput_ids_layer = Input(shape=(max_sequence_length,), dtype=tf.int32, name=\"input_ids\")\nattention_mask_layer = Input(\n    shape=(max_sequence_length,),\n    dtype=tf.int32,\n    name=\"attention_mask\",\n)\n\n# XLM-RoBERTa model for text embeddings\nxlm_roberta_output = xlm_roberta_model(\n    input_ids=input_ids_layer, attention_mask=attention_mask_layer\n)[\n    0\n]  # Use index 0 for the output\nxlm_roberta_output = Flatten()(xlm_roberta_output)  # Flatten the XLM-RoBERTa output\ndropout = Dropout(0.2, name=\"xlm_roberta_dropout\")(xlm_roberta_output)\n\nmerged_bn = BatchNormalization(name=\"merged_batch_norm\")(dropout)\nmerged_dense = Dense(64, activation=\"relu\", name=\"merged_dense\")(merged_bn)\noutput = Dense(1, activation=\"linear\", name=\"output\")(merged_dense)\n\n# Create the final model with descriptive layer names\nmodel = Model(inputs=[input_ids_layer, attention_mask_layer], outputs=output)\n\nmodel.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\nmodel.summary()\n\nhistory = model.fit(\n    train_dataset,\n    epochs=30,\n    batch_size=50,\n    validation_data=test_dataset,\n    callbacks=[early_stopping],\n)","metadata":{"execution":{"iopub.execute_input":"2023-10-14T14:44:59.559372Z","iopub.status.busy":"2023-10-14T14:44:59.559056Z","iopub.status.idle":"2023-10-14T14:46:07.801514Z","shell.execute_reply":"2023-10-14T14:46:07.799340Z","shell.execute_reply.started":"2023-10-14T14:44:59.559343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_epoch = np.argmin(history.history['val_loss'])\nbest_loss = history.history['val_loss'][best_epoch]\nprint(f\"The loss of the best model is: {best_loss}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the testing and training loss.","metadata":{}},{"cell_type":"code","source":"train_loss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\n\nepochs = list(range(1, len(train_loss) + 1))  # Convert range to list\n\nfig = go.Figure()\nfig.add_trace(\n    go.Scatter(\n        x=epochs,\n        y=train_loss,\n        mode=\"lines\",\n        name=\"Training Loss\",\n        line=dict(color=\"blue\"),\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=epochs,\n        y=val_loss,\n        mode=\"lines\",\n        name=\"Validation Loss\",\n        line=dict(color=\"red\"),\n    )\n)\nfig.update_layout(\n    title=\"Training and Validation Loss\",\n    xaxis_title=\"Epochs\",\n    yaxis_title=\"Loss\",\n    showlegend=True,\n    template=\"plotly_white\",\n)\nfig.show()","metadata":{},"execution_count":null,"outputs":[]}]}